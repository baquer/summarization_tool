{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download all the depenndencies beforehand"
      ],
      "metadata": {
        "id": "ZtmaXlM5YT9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#link to the model files.\n",
        "\n",
        "> t5 -  https://drive.google.com/drive/folders/1ikwBy0SKgMuUE8tjfnzxPB1xsanD9TRT?usp=sharing\n",
        "\n",
        "> Pega - https://drive.google.com/drive/folders/1bsHJnV4JwCdaCm5RMgLADYb3sbkXlHex?usp=drive_link\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "15I97Rmcgftk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Youtube Caption Generation."
      ],
      "metadata": {
        "id": "MYVA2VweYYg_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwpzHPFRYDXd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import csv\n",
        "from youtube_transcript_api import YouTubeTranscriptApi as yta\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "api_key = \"YPUTUBE_API_KEY\"\n",
        "\n",
        "# Playlist URL\n",
        "playlist_url = \"https://www.youtube.com/list=PLot-Xpze53lfOdF3KwpMSFEyfE77zIwiP\"\n",
        "\n",
        "# Extract playlist ID from the URL\n",
        "playlist_id = re.findall(r\"list=([^&]+)\", playlist_url)[0]\n",
        "\n",
        "# Initialize YouTube Data API client\n",
        "youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "\n",
        "# Retrieve video IDs from the playlist\n",
        "video_ids = []\n",
        "next_page_token = None\n",
        "while True:\n",
        "    try:\n",
        "        playlist_items = youtube.playlistItems().list(\n",
        "            part=\"contentDetails\",\n",
        "            playlistId=playlist_id,\n",
        "            maxResults=50,\n",
        "            pageToken=next_page_token\n",
        "        ).execute()\n",
        "\n",
        "        video_ids.extend(item[\"contentDetails\"][\"videoId\"] for item in playlist_items[\"items\"])\n",
        "\n",
        "        next_page_token = playlist_items.get(\"nextPageToken\")\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    except HttpError as e:\n",
        "        print(\"An error occurred while retrieving playlist items:\", e)\n",
        "        break\n",
        "with open(\"transcripts.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Index\", \"Content\"])  # Write header row\n",
        "\n",
        "    # Retrieve transcripts for each video\n",
        "    for index, video_id in enumerate(video_ids, start=1):\n",
        "        print(f\"Processing video {index}/{len(video_ids)}\")\n",
        "        try:\n",
        "            # Get transcript for the current video\n",
        "            data = yta.get_transcript(video_id)\n",
        "\n",
        "            if data is not None:\n",
        "                transcript = \"\"\n",
        "\n",
        "                # Extract the text from each caption item\n",
        "                for value in data:\n",
        "                    for key, value in value.items():\n",
        "                        if key == 'text':\n",
        "                            transcript += value + \" \"\n",
        "\n",
        "                # Write the transcript to the CSV file\n",
        "                writer.writerow([index, transcript])\n",
        "\n",
        "            else:\n",
        "                print(f\"Transcripts are disabled for video with ID: {video_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while retrieving transcript for video with ID: {video_id}\")\n",
        "            print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Senetnece Pre-processing"
      ],
      "metadata": {
        "id": "0k_PQcstaugf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
        "    sentence = ' '.join(sentence.split())\n",
        "    tokens = word_tokenize(sentence)\n",
        "    preprocessed_sentence = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_sentence"
      ],
      "metadata": {
        "id": "as4tvkUGYsoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other pre processing were also done like lemmatization, stemming, removing stop words to get the insights of the datset."
      ],
      "metadata": {
        "id": "JosWui5baz_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extractive Summary Generation + Some of the summaries are generated by annotation\n"
      ],
      "metadata": {
        "id": "w_boMUDObUGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in dfnew.iterrows():\n",
        "    content = row['Content']\n",
        "    f=len(content.split())\n",
        "    doc = nlp(content)\n",
        "    tokens=[token.text for token in doc]\n",
        "    word_freq={}\n",
        "    stop_words= list(STOP_WORDS)\n",
        "    for word in doc:\n",
        "      if word.text.lower() not in stop_words:\n",
        "        if word.text.lower() not in punctuation:\n",
        "          if word.text not in word_freq.keys():\n",
        "            word_freq[word.text]= 1\n",
        "          else:\n",
        "            word_freq[word.text]+= 1\n",
        "    x=(word_freq.values())\n",
        "    a=list(x)\n",
        "    a.sort()\n",
        "    max_freq=a[-1]\n",
        "    max_freq\n",
        "    for word in word_freq.keys():\n",
        "      word_freq[word]=word_freq[word]/max_freq\n",
        "    sent_score={}\n",
        "    sent_tokens=[sent for sent in doc.sents]\n",
        "    for sent in sent_tokens:\n",
        "      for word in sent:\n",
        "        if word.text.lower() in word_freq.keys():\n",
        "          if sent not in sent_score.keys():\n",
        "            sent_score[sent]=word_freq[word.text.lower()]\n",
        "          else:\n",
        "            sent_score[sent]+= word_freq[word.text.lower()]\n",
        "    summary=nlargest(n=int(len(sent_score) *0.3),iterable=sent_score,key=sent_score.get)\n",
        "    final_summary=[word.text for word in summary]\n",
        "    f1=[]\n",
        "    for sub in final_summary:\n",
        "      f1.append(re.sub('','',sub))\n",
        "    f2=\" \".join(f1)\n",
        "    dfnew.loc[index, 'summary'] = f2"
      ],
      "metadata": {
        "id": "jtGPzdFhclcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to find the syllogism using the glove embedding"
      ],
      "metadata": {
        "id": "l5YOBW84dTsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unknown_words(text, vocab):\n",
        "    tokens = word_tokenize(text)\n",
        "    unknown = [t for t in tokens if t not in vocab.word2vec]\n",
        "    return unknown"
      ],
      "metadata": {
        "id": "Q_eRUR0zcnG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to check the scores for the validation"
      ],
      "metadata": {
        "id": "dFVbUqgRdtIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate import bleu_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "def calculate_rouge(hypothesis, reference):\n",
        "    hypothesis_tokens = hypothesis.split()\n",
        "    reference_tokens = reference.split()\n",
        "\n",
        "    smoothie = SmoothingFunction().method7\n",
        "\n",
        "    rouge_1_score = bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
        "    rouge_2_score = bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens, weights=(0, 1, 0, 0), smoothing_function=smoothie)\n",
        "    rouge_l_score = bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens, weights=(0, 0, 1, 0), smoothing_function=smoothie)\n",
        "\n",
        "    return rouge_1_score, rouge_2_score, rouge_l_score\n",
        "\n",
        "reference_summary = \"if you have never designed a system before this is probably the place to start so imagine you have a computer with you in which you have written an algorithm so some code is running on this computer and this code gives out an output now people look at this code and they decide that this is really useful to them so they're ready to use that code now you cannot go around giving your computer to everybody so what you do is you expose your code using some protocol which is going to be running on the Internet and by exposing your cool using something called\"\n",
        "generated_summary = dataset1['summary'][0]\n",
        "\n",
        "rouge_1, rouge_2, rouge_l = calculate_rouge(generated_summary, reference_summary)\n",
        "\n",
        "print(\"ROUGE-1 score:\", rouge_1)\n",
        "print(\"ROUGE-2 score:\", rouge_2)\n",
        "print(\"ROUGE-L score:\", rouge_l)\n"
      ],
      "metadata": {
        "id": "53dhHAQ3dnox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and bookmark the results using the markov recorder."
      ],
      "metadata": {
        "id": "iK80ARNaeSyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can toggle the path for trainig the dataset.\n",
        "path = '/content/gdrive/MyDrive/cnn_dailymail/train.csv'\n",
        "\n",
        "# Remove NUll is our dataset\n",
        "# path = '/content/gdrive/MyDrive/RemoveNULL.csv'\n",
        "trainning_df = pd.read_csv(path, engine='python', error_bad_lines=False)\n",
        "\n",
        "MAX_LEN = 512\n",
        "SUMMARY_LEN = 150\n",
        "TRAINNING_SIZE = 1000\n",
        "\n",
        "trainning_df = trainning_df.iloc[0:TRAINNING_SIZE,:].copy()\n",
        "trainning_article_ls = list(trainning_df['article'])\n",
        "trainning_highlight_ls = list(trainning_df['highlights'])\n"
      ],
      "metadata": {
        "id": "XcYKHDtKeLIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['target_text','source_text'])\n",
        "df['target_text'] = trainning_highlight_ls\n",
        "df['source_text'] = ['summarize: '+item for item in trainning_article_ls]"
      ],
      "metadata": {
        "id": "nEiLrujYda8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = \"3tFHvmKR879osa\"\n",
        "try:\n",
        "\tproject = markov.Project.get_by_id(project_id)\n",
        "except markov.exceptions.ResourceNotFoundException:\n",
        "  project = markov.Project(project_name)\n",
        "  project.register()"
      ],
      "metadata": {
        "id": "iTD58kGYfDcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recorder = markov.ExperimentRecorder(\n",
        "    name = \"Summarization Tool3_Rogue\",\n",
        "    note = \"\",\n",
        "    hyper_parameters={\n",
        "        \"lr\": 0.001,\n",
        "        \"momentum\": 0.9,\n",
        "        \"batch_size\": 4,\n",
        "        \"optimizer\": \"ADAM\"\n",
        "    },\n",
        "    project_id=project.project_id\n",
        ")"
      ],
      "metadata": {
        "id": "gsqSErnifHfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to fine tune the dataset\n",
        "recorder.register()\n",
        "with recorder:\n",
        "  model = SimpleT5()\n",
        "  model.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")\n",
        "  MAX_EPOCHS = 2\n",
        "\n",
        "  torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "  torch.utils.checkpoint\n",
        "\n",
        "  model.train(train_df=df[0:(int)(0.7*TRAINNING_SIZE)],\n",
        "              eval_df=df[(int)(0.7*TRAINNING_SIZE):TRAINNING_SIZE],\n",
        "              source_max_token_len=MAX_LEN,\n",
        "              target_max_token_len=SUMMARY_LEN,\n",
        "              batch_size=4, max_epochs=MAX_EPOCHS, use_gpu=True)\n"
      ],
      "metadata": {
        "id": "ibaSGKJ1fN_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with recorder:\n",
        "    for index in sample(list(np.arange(len(trainning_article_ls))), 5):\n",
        "        print('Original Text : ')\n",
        "        print(trainning_article_ls[index])\n",
        "\n",
        "        Original_article = trainning_article_ls[index]\n",
        "\n",
        "        print('\\n\\nSummary Text : ')\n",
        "        print(trainning_highlight_ls[index])\n",
        "\n",
        "        Original_Summary = trainning_highlight_ls[index]\n",
        "\n",
        "        print('\\n\\nFine tuned Predicted Summary Text : ')\n",
        "        model.load_model(\"t5\",\"/content/gdrive/MyDrive/outputs1/simplet5-epoch-1-train-loss-2.844-val-loss-3.2345\", use_gpu=True)\n",
        "        print(model.predict(trainning_article_ls[index]))\n",
        "\n",
        "        Predicted_Summary = trainning_article_ls[index]\n",
        "\n",
        "        rouge = Rouge()\n",
        "        BLEUscore = nltk.translate.bleu_score.sentence_bleu([Predicted_Summary], Original_Summary)\n",
        "\n",
        "        print(\"-----------------\")\n",
        "\n",
        "        print(BLEUscore)\n",
        "        # urid= index\n",
        "        recorder.add_record({\"BLEU Score\": BLEUscore})\n",
        "        recorder.add_record({\"Rogue-1 Recall\": recall_values_r1[0]})\n",
        "        recorder.add_record({\"Rogue-2 Recall\": recall_values_r2[0]})\n",
        "        recorder.add_record({\"Rogue-l Recall\": recall_values_rl[0]})\n",
        "        recorder.add_record({\"Rogue-1 Precision\": precision_values_r1[0]})\n",
        "        recorder.add_record({\"Rogue-2 Precision\": precision_values_r2[0]})\n",
        "        recorder.add_record({\"Rogue-l Precision\": precision_values_rl[0]})\n",
        "        recorder.add_record({\"Rogue-1 F1\": f_values_r1[0]})\n",
        "        recorder.add_record({\"Rogue-2 F1\": f_values_r2[0]})\n",
        "        recorder.add_record({\"Rogue-l F1\": f_values_rl[0]})\n",
        "        print(\"-----------------\")\n",
        "\n",
        "        print('\\n\\nNot Fine tuned Predicted Summary Text : ')\n",
        "        preprocess_text = trainning_article_ls[index].strip().replace(\"\\n\",\"\")\n",
        "        t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "        tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
        "        summary_ids = no_tune_model.generate(tokenized_text,\n",
        "                                            num_beams=4,\n",
        "                                            no_repeat_ngram_size=2,\n",
        "                                            min_length=30,\n",
        "                                            max_length=SUMMARY_LEN,\n",
        "                                            early_stopping=True)\n",
        "        output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        print(output)\n",
        "        print('===========================================================================================================\\n\\n')"
      ],
      "metadata": {
        "id": "T9fse_Y9fZBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same way the pegasus is finetuned and trained on the cnn + own dataset, But nit getting the good results."
      ],
      "metadata": {
        "id": "14pi5ArKgNiB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7c_UfG3PgBAv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}